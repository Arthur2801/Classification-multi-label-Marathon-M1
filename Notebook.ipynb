{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import collections\n",
    "import glob\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,classification_report, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement des données (Ici données confidentiels donc pas sur Github)\n",
    "df= pd.read_csv('challenge_filtered.csv') # Remplacer par le nom ou le chemin du fichier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration / Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''labels=df.iloc[ : , 3:54].columns\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "total_count = []\n",
    "for label in labels:\n",
    "    total_count.append(len(df[df[label] == 1]))\n",
    "ax.bar(labels,total_count, color=['red', 'green', 'blue', 'purple', 'orange', 'yellow'])\n",
    "for i,data in enumerate(total_count):\n",
    "    plt.text(i-.25, \n",
    "              data/total_count[i]+100, \n",
    "              total_count[i], \n",
    "              fontsize=5)\n",
    "plt.title('Number of comments per label')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of comments')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarPlot:\n",
    "    \n",
    "    def __init__(self, df, color):\n",
    "        \"\"\"\n",
    "        Initialise la classe avec un dataframe et une couleur pour le graphique.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.color = color\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Trace un graphique en barres à partir du dataframe et de la couleur donnés en entrée.\n",
    "        \"\"\"\n",
    "        labels = self.df.iloc[:, 2:56].columns # On récupère nos labels\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        total_count = []\n",
    "        for label in labels:\n",
    "            total_count.append(len(self.df[self.df[label] == 1]))\n",
    "\n",
    "        ax.bar(labels, total_count, color=self.color)\n",
    "\n",
    "        #plt.axhline(y=80, color='brown', linestyle='dotted') # Add brown dotted line at y=80\n",
    "\n",
    "        # Add labels to the horizontal line\n",
    "        #ax.text(58, 90, 'Aprés augmentation', fontsize=10, color='brown', ha='left', va='center')\n",
    "        plt.title('Nombre de comptes rendus par codes')\n",
    "        plt.xlabel('Labels')\n",
    "        plt.ylabel('Nombre de comptes rendus')\n",
    "        ax.set_xticklabels([]) # Enlever les étiquettes sur l'axe x\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(df):\n",
    "    # Concaténer tous les textes dans une seule chaîne de caractères\n",
    "    texte = \" \".join(df[\"full_text\"])\n",
    "\n",
    "    # Créer un nuage de mots\n",
    "    nuage_mot = WordCloud(width=800, height=400, background_color=\"white\").generate(texte)\n",
    "\n",
    "    # Afficher le nuage de mots\n",
    "    plt.imshow(nuage_mot)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Preprocessing | splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    # Convertir le texte en minuscule\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer les chiffres qui ne sont pas collés à des lettres\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    # Supprimer la ponctuation\n",
    "    text = re.sub(r'[^\\w\\s]|_', '', text)\n",
    "    \n",
    "    # Tokenization des mots\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Supprimer les stopwords\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatisation des mots\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Rejoindre les tokens pour former une chaîne de caractères\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# On applique la fonction à nos comptes rendus\n",
    "df['full_text'] = df['full_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la colonne contenant le texte des comptes rendus médicaux\n",
    "X = df['full_text']\n",
    "\n",
    "# On récupère toutes les colonnes correspondant aux codes des diagnostics\n",
    "y = df.drop([\"Patient_Id\", \"Admission_Id\", \"age\", \"gender\", \"ethnicity\"], axis=1) \n",
    "\n",
    "# On utilise la méthode MultilabelStratifiedKFold pour diviser les données \n",
    "# tout en veillant à préserver la proportion de chaque classe dans chaque set de données.\n",
    "mskf = MultilabelStratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# On récupère les index des lignes des données qui seront utilisées pour l'entraînement et les tests\n",
    "train_index, test_index = next(mskf.split(X, y))\n",
    "\n",
    "# On récupère les données \n",
    "X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "# On fusionne les données  en une seule dataframe\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# On écrit les données d'entraînement dans un fichier CSV\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "\n",
    "# On écrit les données de test dans un fichier CSV\n",
    "test_df.to_csv('test_data.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest | Paramétres : max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300 (Choisi par GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['427.31', '428.0', '414.01', '412', '410.71', '424.0','427.89', '424.1','427.1', '411.1', '425.4', '414.00', \n",
    "'427.5','428.33', '427.32', '428.32', '428.23', '428.22', '413.9', '414.8',\n",
    "'397.0', '428.30', '427.41', '410.41', '410.11', '426.0', '428.21',\n",
    "'398.91', '414.02', '423.9', '421.0', '428.31', '396.2', '427.81',\n",
    "'428.20', '396.3', '410.91', '428.43', '414.2', '410.72', '426.3',\n",
    "'423.3', '411.89', '424.2', '425.1', '428.42', '426.4', '427.0',\n",
    "'420.90', '429.9', '410.01', '426.11', '426.13', '410.31']\n",
    "class RFModel:\n",
    "    \n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Initialise le modèle de forêt aléatoire avec les données d'entraînement et de test.\n",
    "        \n",
    "        Args:\n",
    "            X_train (array-like): Données d'entraînement.\n",
    "            X_test (array-like): Données de test.\n",
    "            y_train (array-like): Labels des données d'entraînement.\n",
    "            y_test (array-like): Labels des données de test.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.rf = OneVsRestClassifier(RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle de forêt aléatoire sur les données d'entraînement.\n",
    "        \"\"\"\n",
    "        # Vectorisation des données d'entraînement\n",
    "        X_train_vectorized = self.vectorizer.fit_transform(self.X_train)\n",
    "        # Entraînement du modèle sur les données d'entraînement\n",
    "        self.rf.fit(X_train_vectorized, self.y_train)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Évalue le modèle sur les données de test.\n",
    "        \"\"\"\n",
    "        # Vectorisation des données de test\n",
    "        X_test_vectorized = self.vectorizer.transform(self.X_test)\n",
    "        # Prédiction des labels sur les données de test\n",
    "        y_pred = self.rf.predict(X_test_vectorized)\n",
    "        # Calcul des métriques de performance\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        auc = roc_auc_score(self.y_test, y_pred, average='weighted')\n",
    "        hamming = hamming_loss(self.y_test, y_pred)\n",
    "        # Affichage des résultats\n",
    "        CR = classification_report(self.y_test, y_pred, target_names=labels)\n",
    "        print(\"Recall: {:.3f}\".format(recall))\n",
    "        print(\"F1-score: {:.3f}\".format(f1))\n",
    "        print(\"AUC: {:.3f}\".format(auc))\n",
    "        print(\"Hamming Loss: {:.3f}\".format(hamming))\n",
    "        print(CR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM| Paramétres : Kernel : linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMModel:\n",
    "    \n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Initialise le modèle de forêt aléatoire avec les données d'entraînement et de test.\n",
    "        \n",
    "        Args:\n",
    "            X_train (array-like): Données d'entraînement.\n",
    "            X_test (array-like): Données de test.\n",
    "            y_train (array-like): Labels des données d'entraînement.\n",
    "            y_test (array-like): Labels des données de test.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.svm = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle de forêt aléatoire sur les données d'entraînement.\n",
    "        \"\"\"\n",
    "        # Vectorisation des données d'entraînement\n",
    "        X_train_vectorized = self.vectorizer.fit_transform(self.X_train)\n",
    "        # Entraînement du modèle sur les données d'entraînement\n",
    "        self.svm.fit(X_train_vectorized, self.y_train)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Évalue le modèle sur les données de test.\n",
    "        \"\"\"\n",
    "        # Vectorisation des données de test\n",
    "        X_test_vectorized = self.vectorizer.transform(self.X_test)\n",
    "        # Prédiction des labels sur les données de test\n",
    "        y_pred = self.svm.predict(X_test_vectorized)\n",
    "        # Calcul des métriques de performance\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        auc = roc_auc_score(self.y_test, y_pred, average='macro')\n",
    "        hamming = hamming_loss(self.y_test, y_pred)\n",
    "        # Affichage des résultats\n",
    "        CR = classification_report(self.y_test, y_pred, target_names=labels)\n",
    "        print(\"Precision: {:.3f}\".format(precision))\n",
    "        print(\"Recall: {:.3f}\".format(recall))\n",
    "        print(\"F1-score: {:.3f}\".format(f1))\n",
    "        print(\"AUC: {:.3f}\".format(auc))\n",
    "        print(\"Hamming Loss: {:.3f}\".format(hamming))\n",
    "        print(CR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BarPlot2:\n",
    "    \n",
    "    def __init__(self, df, color):\n",
    "        \"\"\"\n",
    "        Initialise la classe avec un dataframe et une couleur pour le graphique.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.color = color\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Trace un graphique en barres à partir du dataframe et de la couleur donnés en entrée.\n",
    "        \"\"\"\n",
    "        labels = self.df.iloc[:, 2:56].columns # On récupère nos labels\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        total_count = []\n",
    "        for label in labels:\n",
    "            count = len(self.df[self.df[label] == 1])\n",
    "            if count < 80:\n",
    "                count = count + (80 - count) # Add up to 80 for counts less than 80\n",
    "            total_count.append(count)\n",
    "\n",
    "        # Color the bars with counts less than 80 in the specified color, and color the bars with augmented counts in blue\n",
    "        colors = [self.color if count < 80 else 'blue' for count in total_count]\n",
    "\n",
    "        ax.bar(labels, total_count, color=colors)\n",
    "\n",
    "        # Add labels to the horizontal line\n",
    "        \n",
    "        plt.title('Nombre de comptes rendus par codes')\n",
    "        plt.xlabel('Labels')\n",
    "        plt.ylabel('Nombre de comptes rendus')\n",
    "        ax.set_xticklabels([]) # Enlever les étiquettes sur l'axe x\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarathonDuWeb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
